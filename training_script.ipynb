{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5a6a2a-8ce1-4e58-bdcb-5034381bb6cd",
   "metadata": {},
   "source": [
    "# Training Script for Resnet on CIFAR-10/CIFAR-\n",
    "\n",
    "\n",
    "Note: this training script is adapted from : https://github.com/gpleiss/temperature_scaling/blob/master/train.py.\n",
    "\n",
    "The original script is used to train a 40-layer DenseNet-BC on CIFAR-100. I have adapted this script for resnet18 and resnet34 on CIFAR-100.\n",
    "\n",
    "Originally, I also tried to train Alexnet on CIFAR-100, but noticed that the accuracy was quite low ~ 54%, so I decided to only use Resnet for my experiments.\n",
    "\n",
    "I also trained Resnet for CIFAR-10, but noticed that the calibration errors were quite low for that, so decided to use CIFAR-100.\n",
    "\n",
    "\n",
    "I trained Resnet18 and Resnet50 for about 50 epochs, which took about total ~2 hour  on Colab on a T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d33c7b-454c-4533-ab2c-cdefb291fccc",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "\n",
    "This training script saves the entire model state and the list of validation indices used from the training set to be used during calibration in the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee7742-caa3-40d0-843c-cb67a350a1f6",
   "metadata": {
    "id": "bcee7742-caa3-40d0-843c-cb67a350a1f6"
   },
   "outputs": [],
   "source": [
    "# Code Libraries needed\n",
    "# Torch and Torchvision (same as main notebook)\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b7df0-e752-426a-a107-4c59aab83ae0",
   "metadata": {
    "id": "4d3b7df0-e752-426a-a107-4c59aab83ae0"
   },
   "outputs": [],
   "source": [
    "def load_data_train_cifar100(train_transform, batch_size=64, valid_size=5000):\n",
    "    train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "    valid_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=False, transform=train_transform)\n",
    "    indices = torch.randperm(len(train_set))\n",
    "    train_indices = indices[:len(indices) - valid_size]\n",
    "    valid_indices = indices[len(indices) - valid_size:] if valid_size else None\n",
    "\n",
    "    # Creating data loaders for train, validation, and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, pin_memory=True, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, pin_memory=True, batch_size=batch_size, sampler=SubsetRandomSampler(valid_indices))\n",
    "    return train_loader, valid_loader, valid_indices\n",
    "\n",
    "def load_data_train_cifar10(train_transform, batch_size=64, valid_size=5000):\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "    valid_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=train_transform)\n",
    "    indices = torch.randperm(len(train_set))\n",
    "    train_indices = indices[:len(indices) - valid_size]\n",
    "    valid_indices = indices[len(indices) - valid_size:] if valid_size else None\n",
    "\n",
    "    # Creating data loaders for train, validation, and test sets\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, pin_memory=True, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, pin_memory=True, batch_size=batch_size, sampler=SubsetRandomSampler(valid_indices))\n",
    "    return train_loader, valid_loader, valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98044d1-ab61-4fe6-97f1-3c09396039bf",
   "metadata": {
    "id": "d98044d1-ab61-4fe6-97f1-3c09396039bf"
   },
   "outputs": [],
   "source": [
    "def get_resnet18_model(num_classes=100):\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=num_classes)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "def get_resnet34_model(num_classes=100):\n",
    "    model = models.resnet34(pretrained=False, num_classes=num_classes)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "def get_resnet50_model(num_classes=100):\n",
    "    model = models.resnet50(pretrained=False, num_classes=num_classes)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LZ49fYD1I4cC",
   "metadata": {
    "id": "LZ49fYD1I4cC"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "  def __init__(self, classes=100, dropout=0.1):\n",
    "    super(AlexNet, self).__init__()\n",
    "    self.features = nn.Sequential(\n",
    "      nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "      nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "      nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=dropout),\n",
    "      nn.Linear(256 * 1 * 1, 4096),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(p=dropout),\n",
    "      nn.Linear(4096, 4096),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Linear(4096, classes),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "def get_alexnet_model():\n",
    "    return AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2496bd-1154-4f11-8f25-8b375df61b71",
   "metadata": {
    "id": "6a2496bd-1154-4f11-8f25-8b375df61b71"
   },
   "outputs": [],
   "source": [
    "# mean = [0.5071, 0.4867, 0.4408]\n",
    "# stdv = [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "# mean and std of CIFAR-10\n",
    "mean_cifar10 = [0.4915, 0.4822, 0.4466]\n",
    "std_cifar10 = [0.2463, 0.2428, 0.2607]\n",
    "\n",
    "# mean and std of CIFAR-100\n",
    "mean_cifar100 = [0.5070, 0.4865, 0.4408]\n",
    "std_cifar100 = [0.2664, 0.2555, 0.2750]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_cifar100, std=std_cifar100),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_cifar100, std=std_cifar100),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9hanA5MgGcRN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hanA5MgGcRN",
    "outputId": "ad6e822e-7d4e-4065-b1ef-6100da17db83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:03<00:00, 42895955.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, valid_indices = load_data_train_cifar100(train_transforms, batch_size=64)\n",
    "# train_loader, valid_loader, valid_indices = load_data_train_cifar10(train_transforms, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077d4c2-c4ba-478b-a23c-43771fd37ab2",
   "metadata": {
    "id": "4077d4c2-c4ba-478b-a23c-43771fd37ab2"
   },
   "outputs": [],
   "source": [
    "class Meter():\n",
    "    \"\"\"\n",
    "    A little helper class which keeps track of statistics during an epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, cum=False):\n",
    "        \"\"\"\n",
    "        name (str or iterable): name of values for the meter\n",
    "            If an iterable of size n, updates require a n-Tensor\n",
    "        cum (bool): is this meter for a cumulative value (e.g. time)\n",
    "            or for an averaged value (e.g. loss)? - default False\n",
    "        \"\"\"\n",
    "        self.cum = cum\n",
    "        if type(name) == str:\n",
    "            name = (name,)\n",
    "        self.name = name\n",
    "\n",
    "        self._total = torch.zeros(len(self.name))\n",
    "        self._last_value = torch.zeros(len(self.name))\n",
    "        self._count = 0.0\n",
    "\n",
    "    def update(self, data, n=1):\n",
    "        \"\"\"\n",
    "        Update the meter\n",
    "        data (Tensor, or float): update value for the meter\n",
    "            Size of data should match size of ``name'' in the initialized args\n",
    "        \"\"\"\n",
    "        self._count = self._count + n\n",
    "        if torch.is_tensor(data):\n",
    "            self._last_value.copy_(data)\n",
    "        else:\n",
    "            self._last_value.fill_(data)\n",
    "        self._total.add_(self._last_value)\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"\n",
    "        Returns the value of the meter\n",
    "        \"\"\"\n",
    "        if self.cum:\n",
    "            return self._total\n",
    "        else:\n",
    "            return self._total / self._count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\t'.join(['%s: %.5f (%.3f)' % (n, lv, v)\n",
    "            for n, lv, v in zip(self.name, self._last_value, self.value())])\n",
    "\n",
    "\n",
    "def run_epoch(loader, model, criterion, optimizer, epoch=0, n_epochs=0, train=True):\n",
    "    time_meter = Meter(name='Time', cum=True)\n",
    "    loss_meter = Meter(name='Loss', cum=False)\n",
    "    error_meter = Meter(name='Error', cum=False)\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "        print('Training')\n",
    "    else:\n",
    "        model.eval()\n",
    "        print('Evaluating')\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(loader):\n",
    "        if train:\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.n_iters = optimizer.n_iters + 1 if hasattr(optimizer, 'n_iters') else 1\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "        # Accounting\n",
    "        _, predictions = torch.max(output, 1)  # Get the indices of the max logits\n",
    "        correct = predictions.eq(target).float().sum()  # Count correct predictions\n",
    "        total = target.size(0)  # Total number of examples\n",
    "        accuracy = correct / total\n",
    "        error = 1 - accuracy\n",
    "        batch_time = time.time() - end\n",
    "        end = time.time()\n",
    "\n",
    "        # Log errors\n",
    "        time_meter.update(batch_time)\n",
    "        loss_meter.update(loss)\n",
    "        error_meter.update(error)\n",
    "        print('  '.join([\n",
    "            '%s: (Epoch %d of %d) [%04d/%04d]' % ('Train' if train else 'Eval',\n",
    "                epoch, n_epochs, i + 1, len(loader)),\n",
    "            str(time_meter),\n",
    "            str(loss_meter),\n",
    "            str(error_meter),\n",
    "        ]))\n",
    "\n",
    "    return time_meter.value(), loss_meter.value(), error_meter.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd46f92-1ca5-4574-8029-6e48e89f059a",
   "metadata": {
    "id": "acd46f92-1ca5-4574-8029-6e48e89f059a"
   },
   "outputs": [],
   "source": [
    "def train(checkpointing_dir, model, model_name, n_epochs, lr=0.01, wd=0.0001, momentum=0.9):\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, nesterov=True)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "    # Train model\n",
    "    best_error = 1\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        scheduler.step()\n",
    "        run_epoch(\n",
    "            loader=train_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            train=True,\n",
    "        )\n",
    "        valid_results = run_epoch(\n",
    "            loader=valid_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            train=False,\n",
    "        )\n",
    "\n",
    "        # Determine if model is the best\n",
    "        _, _, valid_error = valid_results\n",
    "        if valid_error[0] < best_error:\n",
    "            best_error = valid_error[0]\n",
    "            print('New best error: %.4f' % best_error)\n",
    "\n",
    "            # When we save the model, we're also going to include the validation indices\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'best_error': best_error,\n",
    "            }\n",
    "            torch.save(state, os.path.join(checkpointing_dir, f'{model_name}_cifar100.pth'))\n",
    "            #torch.save(model.state_dict(), os.path.join(checkpointing_dir, 'model_resnet18.pth'))\n",
    "            torch.save(valid_indices, os.path.join(checkpointing_dir, f'valid_indices_{model_name}_cifar100.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1047d6-5bf8-46ef-a888-82f2c59cad21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e1047d6-5bf8-46ef-a888-82f2c59cad21",
    "outputId": "4f384eda-cb0c-4162-bf4c-b21e49cede67"
   },
   "outputs": [],
   "source": [
    "model = get_resnet18_model(num_classes=100)\n",
    "checkpointing_dir = \"./trained_models\"\n",
    "if not os.path.exists(checkpointing_dir):\n",
    "        os.makedirs(checkpointing_dir)\n",
    "if not os.path.isdir(checkpointing_dir):\n",
    "    raise Exception('%s is not a dir' % checkpointing_dir)\n",
    "train(checkpointing_dir, model, \"resnet18\", n_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8390c-78ca-4ed3-abea-42826d9d4bfa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
